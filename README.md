Method how to learn a map that takes a content code, derived from a face image, and a randomly chosen style code to an anime image. Derive adversarial loss from simple effective definitions of styles and content. Using this very wide range of anime can be assumptions, the map is not just diverse, but also correctly represents the probability of an anime, conditioned on an input face. Formalization of content and style allows us to create an video Emoji. 


Key results: Here are simple emoji output from the given face image:
![image](https://user-images.githubusercontent.com/93869589/207324821-134b4697-4fbe-4fbd-8f02-f2bd9409f211.png)


Limitations of this project is the below the output Emoji:
![image](https://user-images.githubusercontent.com/93869589/207324882-c15811e0-2bef-4d8c-b652-856b8644b988.png)


Also using this model we can generate female animated faces only not male emoji.
![image](https://user-images.githubusercontent.com/93869589/207324966-e2a4d83d-35d7-4e46-8edf-df23ac0b80e2.png)

The video Animoji of this project output in the code.

Future aspects: 
Create amine dataset of both male and female and using that dataset to create a new gan-model to create Animoji or video Emoji from them.
